@article{Biermann1985,
abstract = {Ten methodologies for automatic program construction are presented, discussed and compared. Some of the techniques generate code from formal input-output specifications while others work from examples of the target behaviour or from natural language input.},
author = {Biermann, Alan W.},
doi = {10.1016/S0747-7171(85)80010-9},
file = {:tmp/mozilla{\_}vasilis0/1-s2.0-S0747717185800109-main.pdf:pdf},
issn = {07477171},
journal = {Journal of Symbolic Computation},
number = {2},
pages = {119--142},
title = {{Automatic programming : A tutorial on formal methodologies}},
url = {http://www.sciencedirect.com/science/article/pii/S0747717185800109},
volume = {1},
year = {1985}
}
@article{Schmidt2006,
abstract = {Model-driven engineering technologies offer a promising approach to address the inability of third-generation languages to alleviate the complexity of platforms and express domain concepts effectively. Over the past five decades, software researchers and developers have been creating abstractions that help them program in terms of their design intent rather than the underlying computing environment—for example, CPU, memory, and network devices—and shield them from the complexities of these environments. From the early days of computing, these abstractions included both language and platform technologies. For example, early programming languages, such as assembly and Fortran, shielded developers from complexities of programming with machine code. Likewise, early operating system platforms, such as OS/360 and Unix, shielded developers from complexities of programming directly to hardware. Although these early languages and platforms raised the level of abstraction, they still had a distinct "computing-oriented" focus. In particular, they provided abstractions of the solution space—that is, the domain of computing technologies themselves—rather than abstractions of the problem space that express designs in terms of concepts in application domains, such as telecom, aerospace, healthcare, insurance, and biology. LESSONS FROM COMPUTER-AIDED SOFTWARE ENGINEERING Various past efforts have created technologies that further elevated the level of abstraction used to develop software. One prominent effort begun in the 1980s was computer-aided software engineering (CASE), which focused on developing software methods and tools that enabled developers to express their designs in terms of general-purpose graphical programming representations, such as state machines, structure diagrams, and dataflow diagrams. One goal of CASE was to enable more thorough analysis of graphical programs that incur less complexity than conventional general-purpose programming languages—for example, by avoiding memory corruption and leaks associated with languages like C. Another goal was to synthesize implementation artifacts from graphical representations to reduce the effort of manually coding, debugging, and porting programs. Although CASE attracted considerable attention in the research and trade literature, it wasn't widely adopted in practice. One problem it faced was that the general-purpose graphical language representations for writing programs in CASE tools mapped poorly onto the underlying platforms, which were largely single-node operating systems—such as DOS, OS/2, or Windows—that lacked support for important quality-of-service (QoS) properties, such as transparent distribution, fault tolerance, and security. The amount and complexity of generated code needed to compensate for the paucity of the underlying platforms was beyond the grasp of translation technologies available at the time, which made it hard to develop, debug, and evolve CASE tools and applications created with these tools. Another problem with CASE was its inability to scale to handle complex, production-scale systems in a broad range of application domains. In general, CASE tools did not support concurrent engineering, so they were limited to programs written by a single person or by a team that serialized their access to files used by these tools. Moreover, due to a lack of powerful common middleware platforms, CASE tools targeted proprietary execution environments, which made it hard to integrate the code they generated with other software language and platform technologies. CASE tools also didn't support many application domains effectively because their "one-size-fits-all" graphical representations were too generic and noncustomizable. As a result, CASE had relatively little impact on commercial software development during the 1980s and 1990s, focusing primarily on a few domains, such as telecom call processing, that mapped nicely onto state machine representations. To},
author = {Schmidt, Douglas C},
doi = {10.1109/MC.2006.58},
file = {:tmp/mozilla{\_}vasilis0/10.1.1.106.9720.pdf:pdf},
isbn = {0018-9162},
issn = {00189162},
journal = {IEEE Computer},
number = {2},
pages = {25--31},
pmid = {19417437},
title = {{Model-Driven Engineering}},
url = {http://www.computer.org/portal/site/computer/menuitem.e533b16739f5...},
volume = {39},
year = {2006}
}
@article{LeCun2015,
abstract = {Deep learning allows computational models that are composed of multiple processing layers to learn representations of data with multiple levels of abstraction. These methods have dramatically improved the state-of-the-art in speech recognition, visual object recognition, object detection and many other domains such as drug discovery and genomics. Deep learning discovers intricate structure in large data sets by using the backpropagation algorithm to indicate how a machine should change its internal parameters that are used to compute the representation in each layer from the representation in the previous layer. Deep convolutional nets have brought about breakthroughs in processing images, video, speech and audio, whereas recurrent nets have shone light on sequential data such as text and speech.},
archivePrefix = {arXiv},
arxivId = {arXiv:1312.6184v5},
author = {LeCun, Yann and Bengio, Yoshua and Hinton, Geoffrey},
doi = {10.1038/nature14539},
eprint = {arXiv:1312.6184v5},
file = {:home/vasilis/Desktop/deep-learning-nature2015.pdf:pdf},
isbn = {9780521835688},
issn = {0028-0836},
journal = {Nature},
number = {7553},
pages = {436--444},
pmid = {10463930},
title = {{Deep learning}},
url = {http://dx.doi.org/10.1038/nature14539},
volume = {521},
year = {2015}
}
@article{Parnas1985,
abstract = {A former member of the SDIO Panel on Computing in Support of Battle Management explains why he believes the “Star Wars” effort will not achieve its stated goals.},
author = {Parnas, David Lorge},
doi = {10.1145/382288.382289},
isbn = {0001-0782},
issn = {01635948},
journal = {ACM SIGSOFT Software Engineering Notes},
number = {12},
pages = {15--23},
title = {{Software aspects of strategic defense systems}},
volume = {10},
year = {1985}
}
@article{Graves2013,
abstract = {This paper shows how Long Short-term Memory recurrent neural networks can be used to generate complex sequences with long-range structure, simply by predicting one data point at a time. The approach is demonstrated for text (where the data are discrete) and online handwriting (where the data are real-valued). It is then extended to handwriting synthesis by allowing the network to condition its predictions on a text sequence. The resulting system is able to generate highly realistic cursive handwriting in a wide variety of styles.},
archivePrefix = {arXiv},
arxivId = {arXiv:1308.0850v5},
author = {Graves, Alex},
doi = {10.1145/2661829.2661935},
eprint = {arXiv:1308.0850v5},
file = {:home/vasilis/Dropbox/2015-10 Bountris - RNN source code/Resources/Generating Sequences With RNNs - Alex Graves.pdf:pdf},
isbn = {2000201075},
issn = {18792782},
journal = {Technical Reports},
pages = {1--43},
pmid = {23459267},
title = {{Generating Sequences with Recurrent Neural Networks}},
url = {http://arxiv.org/abs/1308.0850},
year = {2013}
}