\chapter{Σχετική βιβλιογραφία}

Στο κεφάλαιο αυτό παρουσιάζουμε σχετικές προσεγγίσεις και υλοποιήσεις στο πρόβλημα του αυτόματου προγραμματισμού και της παραγωγής κώδικα.
Επειδή είναι πρακτικά αναρίθμητες, θα επικεντρωθούμε σε αυτές που είναι σχετικές με τα αναδραστικά νευρωνικά δίκτυα και σε κάποιες που παρουσιάζουν ιδιαίτερο ενδιαφέρον.

\section{\en{Generating Sequence with Recurrent Neural Networks}}
Στην εργασία των \en{Graves et al.} παρουσιάζεται πως απλές δομές αναδραστικών νευρωνικών δικτύων με στοιχεία μνήμης \en{LSTM} μπορούν να χρησιμοποιοηθούν για να παράξουν σύνθετες ακολουθίες, απλά προβλέποντας ένα στοιχείο της ακολουθίας τη φορά. 
Θεωρώντας τις προβλέψεις στοχαστικές, καινούριες ακολουθίες μπορούν να προκύψουν απο ένα εκπαιδευμένο δίκτυο, δειγματοληπτώντας επαναληπτικά από την έξοδο του δικτύου και ύστερα ξαναδίνοντας ως είσοδο στο δίκτου την δειγματοληπτημένη πρόβλεψη.
Με μία διαφορετική διατύπωση, αφήνουμε το δίκτυο να αντιμετωπίσει τις επινοήσεις του ως αληθινές, περίπου σαν έναν άνθρωπο ο οποίος ονειρεύεται. 
Αν και το σύστημα είναι ντερεμινιστικό, η στοχαστικότητα που εισάγεται δειγματοληπτώντας δημιουργεί μία κατανομή σε σχέση με τις ακολουθίες.
Αυτή η κατανομή είναι δεσμευμένη, αφού η εσωτερική αναπαράσταση του δικτύου, άρα και κατανομή προβλέψεων του, εξαρτάται από τις προηγούμενες εισόδους.

Η προσέγγιση αυτή επιδεικνύεται για κείμενο (όπου οι τιμές είναι διακριτές) και για <<\en{online}>>  χειρόγραφο κείμενο (όπου οι τιμές είναι πραγματικές).
Με τον όρο <<\en{online}>> εννοούμε οτι η γραφή αποτυπώνεται ως ακολουθία διάνυσματων θέσης ενός μολυβιού -- σε αντίθεση με το <<\en{offline}>> στο οποίο έχουμε διαθέσιμη ολόκληρη την εικόνα του χειρόγραφου.
Το σύστημα που χρησιμοποιείται είναι μια συστάδα που αποτελείται απο 7 επίπεδα αναδραστικών νευρωνικών δικτυών με 700 στοχεία μνήμης \en{LSTM}.
Για την παραγωγή ακολουθιών κειμένου χρησιμοποιούνται τρία διαφορετικά σετ δεδομένων. Το Penn Treebank και το Wikipedia Hutter Prize για το γραπτό κείμενο και το \en{<<IAM online handwriting database>>}.
Το μοντέλο καταφέρνει να παράξει ακολουθιές τόσο ρεαλιστικές ώστε να είναι συχνά δύσκολο να τις ξεχωρίσει κανείς από πραγματικές, τουλάχιστον σε πρώτη όψη.
Στα αποτελέσματα είναι ορατή μια μεγάλης εμβέλειας δομή και συνοχή.

\begin{figure}[tph]
	\includegraphics[width=\textwidth, keepaspectratio]{images/handwriting.png}
	\centering 
	\caption{Ένα τυπικό δομικό διάγραμμα επιτηρούμενης εκμάθησης.}
	\label{fig:training}
\end{figure}

Επιπρόσθετα, βασισμένοι στην προηγούμενη δομη, οι \en{Graves et al.} σχεδιάζουν ένα σύστημα παραγωγής χειρόγραφου κειμένου το οποίο μπορεί να γράψει αυτό που του ζητάμε.
Αυτό γίνεται με την προσθήκη ενός διανύσματος της πρότασης που θέλουμε να γράψουμε, το οποίο δίνεται στο σύστημα πρόβλεψης την ώρα της παραγωγής, αφού προφανώς εκπαιδευτεί σε σχετικά προβλήματα.
Η απόφαση για το πότε και πως θα γραφεί κάθε χαρακτήρας αφήνεται στο νευρωνικό δίκτυο και τα αποτελέσματα είναι αρκετά ικανοποιητικά ώστε να είναι και πάλι δύσκολο να διακριθεί αν τα <<χειρόγραφα>> ανήκουν σε κάποιον άνθρωπο ή στο σύστημα.

\section{\en{Inferring Algorithmic Patterns with Stack-Augmented Recurrent Nets}}

Οι \en{Joulin et al.} στην έρευνα τους εξετάζουν τα όρια των \en{"state of the art" Deep Learning} προσεγγίσεων.
Πιο συγκεκριμένα, εξετάζονται τα απλούστερα προβλήματα πρόβλεψης ακολουθιών που είναι πέρα από τις δυνατότητες εκμάθησης των τυπικών αναδραστικών δικτύων: αλγοριθμικά παραγμένες ακολουθίες που μπορούν να μαθευτούν μόνο απο συστήματα με δυνατότητα μνήμης και απαρίθμησης.
Για παράδειγμα η σχέση $a^nb^n, n > 0$ μπορεί να παράξει την ακολουθία \en{aab\textbf{ba}aab\textbf{bba}b\textbf{a}aaaab\textbf{bbbb}}, όπου με έντονη γραμματοσειρά σημειώνονται τα στοιχεία της ακολουθίας που μπορούν να προβλεφθούν ντετερμενιστικά.

Εξετάζονται 4 διαφορετικά μοντέλα: ένα απλό αναδραστικό νευρωνικό δίκτυο, ένα \en{RNN} με στοιχεία μνήμης \en{LSTM} και 2 \en{RNN} με εξωτερική μνήμη.
Η εξωτερική μνήμη είναι για το ένα μοντέλο μια διπλά συνδεδεμένη λίστα και για το άλλο μοντέλο μια στοίβα. Όλα τα μοντέλα εκπαιδεύονται με τον αλγόριθμο \en{SGD}.
Τα μοντέλα με την εξωτερική μνήμη μαθαίνουν να χρησιμοποιούν τις θεωρητικά απείρου μήκους εξωτερικές μνήμες τους, με στοιχειώδεις εντολές (\en{push, pop, insert, no-op}). 

Τελικώς δείχνουν πως μερικοί βασικοί αλγόριθμοι μπορούν να μαθευτούν απο ακολουθιακά δεδομένα χρησιμοποιώντας \en{RNNs} με μνήμη. Τα μοντέλα με εξωτερική μνήμη ξεπερνούν σε επιδόσεις τα υπόλοιπα μοντέλα. Οι συγγραφείς υποσημειώνουν πως είναι σημαντικό να μεγαλώσουμε την πολυπλοκότητα του μοντέλου με δομημένο τρόπο και πως η δομή των νευρωνικών δικτύων θα πρέπει να μαθαίνεται απο τα δεδομένα και να μην προαποφασίζεται.


\section{\en{A Synthetic Neural Model for General Purpose Code Generation}}
